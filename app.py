
import streamlit as st
st.set_page_config(page_title="QuLab", layout="wide")
st.sidebar.image("https://www.quantuniversity.com/assets/img/logo5.jpg")
st.sidebar.divider()
st.title("QuLab")
st.divider()
st.markdown("""
In this lab, we explore "Agentic AI for Safety Monitoring" by providing a practical, end-to-end introduction to Explainable AI (XAI) for Large Language Models (LLMs). We leverage synthetic data and lightweight visualizations to demonstrate core XAI concepts and their practical applications in safety monitoring. This application will guide you through understanding interpretability vs. transparency, generating synthetic LLM interaction data, simulating saliency maps and counterfactuals, and analyzing faithfulness over time.

### Learning Goals:
- Understand key XAI concepts for LLMs, including interpretability vs. transparency, and their relevance for risk, trust, and governance.
- Generate realistic synthetic datasets of LLM interactions to explore XAI signals at scale.
- Simulate and visualize cornerstone XAI techniques: saliency maps and counterfactuals.
- Analyze faithfulness over time and understand the trade-off between explanation quality and model accuracy.
- Apply practical filtering mechanisms (e.g., by explanation verbosity and model confidence) to focus reviews on high-value cases.
- Design testing and validation for adaptive systems and apply explainability frameworks to LLMs.
- Understand the importance of XAI for AI-security threats and implementing defenses in agentic systems.

### Key Formulae:
- **Saliency importance (conceptual)**: $ S(x_i) = \left| \frac{\partial Y}{\partial x_i} \right| $ captures how sensitive the model output $Y$ is to small changes in token $x_i$. High $|\partial Y/\partial x_i|$ implies stronger influence, guiding auditors to critical tokens.
- **Counterfactual minimal change**: Find $X' = X + \Delta X$ such that $\text{Model}(X') = Y' \ne Y$ and $||\Delta X||$ is minimal. This supports "what-if" analysis for decision recourse and fairness reviews.
- **Accuracy (simulated binary)**: $ A = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}[\hat{y}_i = y_i] $ Even when simplified, plotting $A$ against explanation quality $Q_{exp}$ surfaces potential trade-offs.

Business linkage:
- Explainability reduces operational risk (model audits), accelerates root-cause analysis (incident response), and improves user trust (comms & compliance). The visuals we produce are the backbone of explainability reporting in production dashboards.
""")
# Your code starts here
page = st.sidebar.selectbox(label="Navigation", options=["Data Generation & Validation", "XAI Techniques", "Trend & Trade-off Analysis"])
if page == "Data Generation & Validation":
    from application_pages.page1 import run_page1
    run_page1()
elif page == "XAI Techniques":
    from application_pages.page2 import run_page2
    run_page2()
elif page == "Trend & Trade-off Analysis":
    from application_pages.page3 import run_page3
    run_page3()
# Your code ends


# License
st.caption('''
---
## QuantUniversity License

Â© QuantUniversity 2025  
This notebook was created for **educational purposes only** and is **not intended for commercial use**.  

- You **may not copy, share, or redistribute** this notebook **without explicit permission** from QuantUniversity.  
- You **may not delete or modify this license cell** without authorization.  
- This notebook was generated using **QuCreate**, an AI-powered assistant.  
- Content generated by AI may contain **hallucinated or incorrect information**. Please **verify before using**.  

All rights reserved. For permissions or commercial licensing, contact: [info@qusandbox.com](mailto:info@qusandbox.com)
''')
